{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from scipy.ndimage import gaussian_filter1d,uniform_filter1d\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from scipy.special import huber\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据并滤波"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./loss curves/gpt_loss+lrs.pkl')\n",
    "df_811 = df['M:100M_gpt_D:20B_scheduler:811_rope']\n",
    "df_wsd = df['M:100M_gpt_D:20B_scheduler:wsd_rope']\n",
    "df_cos = df['M:100M_gpt_D:20B_scheduler:cosine_rope']\n",
    "\n",
    "df_811['S1'] = np.cumsum(df_811['lr'])\n",
    "df_wsd['S1'] = np.cumsum(df_wsd['lr'])\n",
    "df_cos['S1'] = np.cumsum(df_cos['lr'])\n",
    "\n",
    "# 滤波\n",
    "\n",
    "# data_811['loss_filter']=uniform_filter1d(data_811['Metrics/loss'], size=5)\n",
    "# data_wsd['loss_filter']=uniform_filter1d(data_wsd['Metrics/loss'], size=5)\n",
    "# data_cos['loss_filter']=uniform_filter1d(data_cos['Metrics/loss'], size=5)\n",
    "\n",
    "df_811['loss_filter']=gaussian_filter1d(df_811['Metrics/loss'], sigma=1.5)\n",
    "df_wsd['loss_filter']=gaussian_filter1d(df_wsd['Metrics/loss'], sigma=1.5)\n",
    "df_cos['loss_filter']=gaussian_filter1d(df_cos['Metrics/loss'], sigma=1.5)\n",
    "\n",
    "print(df_811.shape,df_811.head())\n",
    "a=df_811['Metrics/loss'].to_numpy()\n",
    "a[torch.tensor([0,1],dtype=torch.int32)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型 (Tissue2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_tissue(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        L0,A,alpha,C,lamb\n",
    "    \"\"\"\n",
    "    def __init__(self, df_train: pd.DataFrame,\n",
    "                L0: float, A: float, alpha: float,\n",
    "                C: float, lamb: float):\n",
    "        super().__init__()\n",
    "        self.L0 = nn.Parameter(torch.tensor(L0, dtype=torch.float64))\n",
    "        self.A = nn.Parameter(torch.tensor(A, dtype=torch.float64))\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha, dtype=torch.float64))\n",
    "        self.C = nn.Parameter(torch.tensor(C, dtype=torch.float64))\n",
    "        self.lamb = nn.Parameter(torch.tensor(lamb, dtype=torch.float64))\n",
    "        self.data_lr = torch.from_numpy(df_train['lr'].to_numpy())\n",
    "        self.data_S1 = torch.from_numpy(df_train['S1'].to_numpy())\n",
    "        self.data_loss = torch.from_numpy(df_train['loss_filter'].to_numpy())\n",
    "\n",
    "    def forward(self,step_batch: torch.tensor):\n",
    "        S1 = self.data_S1[step_batch]\n",
    "        S2 = torch.zeros_like(step_batch, dtype=torch.float64)\n",
    "        for i, s in enumerate(step_batch):\n",
    "            # 计算S2\n",
    "            j_indices = torch.arange(1,s+1,1,dtype=torch.int32)\n",
    "            S2[i] = torch.sum( # sigma _j=1 ^s\n",
    "                (self.data_lr[j_indices-1]-self.data_lr[j_indices])*(1-torch.pow(self.lamb,s+1-j_indices))/(1-self.lamb)\n",
    "                )\n",
    "        pred = self.L0 + self.A * S1 ** (-self.alpha) - self.C * S2\n",
    "        r = torch.log(self.data_loss[step_batch]) - torch.log(pred.clamp(min=1e-10))  # Avoid log(0)\n",
    "\n",
    "        # huber loss\n",
    "        delta = 1e-3\n",
    "        huber_loss = torch.where(torch.abs(r) < delta, 0.5 * r ** 2, delta * (torch.abs(r) - 0.5 * delta))\n",
    "        return huber_loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型参数初始化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params_tissue(df_train: pd.DataFrame, step_batch: list):\n",
    "    min_loss = df_train['loss_filter'].min()\n",
    "    log_y = np.log(df_train[\"loss_filter\"][step_batch] - min_loss + 1e-3)\n",
    "    log_x = np.log(df_train[\"S1\"][step_batch])\n",
    "    slope, intercept, _, _, _ = linregress(log_x, log_y)\n",
    "\n",
    "    L0_init_set = np.linspace(min_loss - 0.2, min_loss + 0.2, 5)\n",
    "    A_init_set = np.linspace(np.exp(intercept) - 0.1, np.exp(intercept) + 0.1, 3)\n",
    "    alpha_init_set = np.linspace(-slope - 0.1, -slope + 0.1, 3)\n",
    "    C_init_set = np.linspace(100, 1000, 3)\n",
    "\n",
    "    def loss_fn0(params):\n",
    "        L0, A, alpha, C = params\n",
    "        pred = L0 + A * df_train[\"S1\"][step_batch] ** (-alpha) - C * (3e-4 - df_train[\"lr\"][step_batch])\n",
    "        r = np.log(df_train[\"loss_filter\"][step_batch]) - np.log(pred+1e-10)\n",
    "        return huber(1e-3,r).sum()\n",
    "\n",
    "    init_params = list(product(L0_init_set, A_init_set, alpha_init_set, C_init_set))\n",
    "    best_loss = float('inf')\n",
    "    best_params = None\n",
    "\n",
    "    for init_param in tqdm(init_params, desc=\"Initializing Parameters\"):\n",
    "        res = minimize(\n",
    "            loss_fn0, init_param, method='L-BFGS-B', bounds=[(0, np.inf)] * 4,\n",
    "            options={'maxiter': 100000, 'ftol': 1e-9, 'gtol': 1e-6, 'eps': 1e-8}\n",
    "        )\n",
    "        if res.fun < best_loss:\n",
    "            best_loss = res.fun\n",
    "            best_params = res.x\n",
    "    return dict(zip(['L0', 'A', 'alpha', 'C'],best_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(model,step_batch,max_step=200,loss_thr=1e-10,patience=20,grad_norm_thr=1e-5):\n",
    "    optimizer = torch.optim.AdamW([\n",
    "            {\"params\": [model.L0, model.A, model.C], \"lr\": 0.05},\n",
    "            {\"params\": [model.alpha, model.lamb], \"lr\": 0.005},\n",
    "        ])\n",
    "    loss_history, min_loss, steps_no_improve = [], float('inf'), 0\n",
    "    best_params, best_loss = None, float('inf')\n",
    "\n",
    "    for _ in tqdm(range(max_step), desc=\"Training Progress\"):\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = model(step_batch)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_history.append(total_loss.item())\n",
    "\n",
    "        if total_loss < min_loss - loss_thr:\n",
    "            min_loss = total_loss.item()\n",
    "            steps_no_improve = 0\n",
    "        else:\n",
    "            steps_no_improve += 1\n",
    "\n",
    "        if steps_no_improve >= patience:\n",
    "            break\n",
    "\n",
    "        grads = [p.grad.flatten() for p in model.parameters() if p.grad is not None]\n",
    "        grad_norm = torch.cat(grads).norm() if grads else torch.tensor(0.0)\n",
    "        if grad_norm < grad_norm_thr:\n",
    "            break\n",
    "\n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss.item()\n",
    "            best_params = [p.item() for p in model.parameters()]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(np.arange(len(loss_history)), loss_history, label=\"Fitting loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"./tissue_fit_monitor.png\")\n",
    "\n",
    "    return best_params, best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义后处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_eval_tissue:\n",
    "    def __init__(self, df: pd.DataFrame, fig_id : str,\n",
    "                L0: float, A: float, alpha: float,\n",
    "                C: float, lamb: float,):\n",
    "        super().__init__()\n",
    "        self.fig_id = fig_id\n",
    "        self.L0 = (torch.tensor(L0, dtype=torch.float64))\n",
    "        self.A = (torch.tensor(A, dtype=torch.float64))\n",
    "        self.alpha = (torch.tensor(alpha, dtype=torch.float64))\n",
    "        self.C = (torch.tensor(C, dtype=torch.float64))\n",
    "        self.lamb = (torch.tensor(lamb, dtype=torch.float64))\n",
    "        self.data_lr = torch.from_numpy(df['lr'].to_numpy())\n",
    "        self.data_S1 = torch.from_numpy(df['S1'].to_numpy())\n",
    "        self.data_loss = torch.from_numpy(df['loss_filter'].to_numpy())\n",
    "        self.data_loss_ori = torch.from_numpy(df['Metrics/loss'].to_numpy())\n",
    "\n",
    "    def forward(self,step_batch:torch.tensor):\n",
    "        S1 = self.data_S1[step_batch]\n",
    "        S2 = torch.zeros_like(step_batch, dtype=torch.float64)\n",
    "        for i, s in enumerate(step_batch):\n",
    "            # 计算S2\n",
    "            j_indices = torch.arange(1,s+1,1,dtype=torch.int32)\n",
    "            S2[i] = torch.sum( # sigma _j=1 ^s\n",
    "                (self.data_lr[j_indices-1]-self.data_lr[j_indices])*(1-torch.pow(self.lamb,s+1-j_indices))/(1-self.lamb)\n",
    "                )\n",
    "        pred = self.L0 + self.A * S1 ** (-self.alpha) - self.C * S2\n",
    "        r = torch.log(self.data_loss[step_batch]) - torch.log(pred.clamp(min=1e-10))  # Avoid log(0)\n",
    "\n",
    "        # huber loss\n",
    "        delta = 1e-3\n",
    "        huber_loss = torch.where(torch.abs(r) < delta, 0.5 * r ** 2, delta * (torch.abs(r) - 0.5 * delta))\n",
    "\n",
    "        # 绘图\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(step_batch, self.data_loss_ori[step_batch], label=f\"{self.fig_id}: original\", linestyle=\"-\",linewidth=1.5)\n",
    "        plt.plot(step_batch, self.data_loss[step_batch], label=f\"{self.fig_id}: filtered\", linestyle=\"--\",linewidth=1.5)\n",
    "        plt.plot(step_batch, pred, label=f\"{self.fig_id}: predict\", linestyle=\"--\",linewidth=1.5)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f\"LRS: {self.fig_id}\")\n",
    "        plt.savefig(f\"tissue_eval_{self.fig_id}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        return huber_loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义LRS优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrs_opt(L0, A, alpha, C, lamb,\n",
    "        total_steps=24000,\n",
    "        peak_lr=3e-4,\n",
    "        min_lr=1e-10,\n",
    "        lr=5e-9,\n",
    "        max_steps=10000,\n",
    "        warmup=2160,\n",
    "        ):\n",
    "\n",
    "    '''\n",
    "    Args:\n",
    "        Fitted MPL parameters [L0, A, alpha, C, lamb].\n",
    "        total_steps (int): Total steps in the schedule.\n",
    "        peak_lr (float): Initial peak learning rate.\n",
    "        min_lr (float): Minimum learning rate threshold.\n",
    "        lr (float): Learning rate for optimization.\n",
    "        max_steps (int): Maximum optimization steps.\n",
    "        warmup (int): Number of warmup steps.\n",
    "        name (str): Identifier for output files.\n",
    "    '''\n",
    "\n",
    "    # Initialize Delta (learnable LR reductions)\n",
    "    delta = nn.Parameter(torch.zeros(total_steps - warmup, dtype=torch.float64), requires_grad=True)\n",
    "    warmup_bias = 0.5 * peak_lr * warmup\n",
    "    optimizer = torch.optim.Adam([delta], lr=lr)\n",
    "    '''\n",
    "            S1 = self.data_S1[step_batch]\n",
    "            S2 = torch.zeros_like(step_batch, dtype=torch.float64)\n",
    "            for i, s in enumerate(step_batch):\n",
    "                # 计算S2\n",
    "                j_indices = torch.arange(1,s+1,1,dtype=torch.int32)\n",
    "                S2[i] = torch.sum( # sigma _j=1 ^s\n",
    "                    (self.data_lr[j_indices-1]-self.data_lr[j_indices])*(1-torch.pow(self.lamb,s+1-j_indices))/(1-self.lamb)\n",
    "                    )\n",
    "            pred = self.L0 + self.A * S1 ** (-self.alpha) - self.C * S2\n",
    "    '''\n",
    "    # Optimization loop\n",
    "    for _ in tqdm(range(max_steps), desc=\"Optimizing LR Schedule\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute LR schedule from Delta\n",
    "        eta = peak_lr - torch.cumsum(delta.clamp(min=0), dim=0)\n",
    "        eta = torch.clamp(eta, min=min_lr)\n",
    "\n",
    "        S1 = torch.cumsum(eta, dim=0) + warmup_bias\n",
    "        S1 = torch.concatenate([torch.tensor([0]), S1], dim=0)\n",
    "\n",
    "        s = total_steps - warmup -1\n",
    "\n",
    "        j_indices = torch.arange(1,s+1,1,dtype=torch.int32)\n",
    "        S2 = torch.sum( # sigma _j=1 ^s\n",
    "            (eta[j_indices-1]-eta[j_indices])*(1-torch.pow(lamb,s+1-j_indices))/(1-lamb)\n",
    "            )\n",
    "\n",
    "        pred = L0 + A * S1[-1] ** (-alpha) - C * S2\n",
    "        pred.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Enforce constraints\n",
    "        with torch.no_grad():\n",
    "            delta.clamp_(min=0, max=peak_lr)\n",
    "            eta = peak_lr - torch.cumsum(delta, dim=0)\n",
    "            delta.masked_fill_(eta <= min_lr, 0)\n",
    "            opt_lr = eta.detach().numpy()\n",
    "            loss = pred.item()\n",
    "\n",
    "    # 优化结果: opt_lr -> np.save('luo_opt_lr.npy', opt_lr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(np.arange(warmup, total_steps), opt_lr)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Learning rate\")\n",
    "    plt.title(f\"Optimized learning rate schedule\")\n",
    "    plt.savefig(\"tissue_opt_lr.png\")\n",
    "    plt.close()\n",
    "    print(f\"Final loss for optimal LRS: {loss}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主函数：一个用于训练，两个用于测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df_train,df_test1,df_test2,num_steps=100):\n",
    "    \"\"\"\n",
    "    Main function to run the model fitting and evaluation.\n",
    "    num_steps(int): Number of steps for training.\n",
    "    \"\"\"\n",
    "    print(f\"step.min()={df_train['step'].min()}, step.max()={df_train['step'].max()}, num_steps={num_steps}\")\n",
    "    step_batch = np.linspace(df_train['step'].min()+1, df_train['step'].max()-1, num_steps, dtype=int)\n",
    "\n",
    "    # 参数初始化\n",
    "    init_params = initialize_params_tissue(df_train,step_batch)\n",
    "    init_params['lamb'] = 0.995\n",
    "    model=Model_tissue(df_train,**init_params)\n",
    "\n",
    "    # 模型拟合 best_params = [L0, A, alpha, C, lamb]\n",
    "    step_batch = torch.tensor(step_batch,dtype=torch.int32)\n",
    "    best_params, best_loss = model_fit(model,step_batch)\n",
    "\n",
    "    # 检验拟合结果\n",
    "    m1 = Model_eval_tissue(df_train,'811',*best_params)\n",
    "    m2 = Model_eval_tissue(df_test1,'cos',*best_params)\n",
    "    m3 = Model_eval_tissue(df_test2,'wsd',*best_params)\n",
    "    m1.forward(torch.arange(1,30000,100, dtype=torch.int32))\n",
    "    m2.forward(torch.arange(1,30000,100, dtype=torch.int32))\n",
    "    m3.forward(torch.arange(1,30000,100, dtype=torch.int32))\n",
    "\n",
    "    # LRS优化\n",
    "    warmup = 2000\n",
    "    total_steps = df_train['step'].max() + 1 + warmup\n",
    "    lrs_opt(*best_params,warmup=warmup,total_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(df_train=df_811, df_test1=df_cos, df_test2=df_wsd, num_steps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
